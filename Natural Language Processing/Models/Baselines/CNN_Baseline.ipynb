{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word2Vect_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK3cBHOb5-OC"
      },
      "source": [
        "# CNN Model with Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZnyqoqO6fdC",
        "outputId": "a22f72d1-1f4f-46c5-ccc1-09b4df601427"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVz5RwLX5-OJ",
        "outputId": "a4dd2475-165b-43b6-a48c-a725174076d9"
      },
      "source": [
        "#user defined module\n",
        "! pip install alibi\n",
        "!cat '/content/gdrive/My Drive/w266_bot_id_final_project/bot_id_pan/load_data.py'\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/gdrive/My Drive/w266_bot_id_final_project/bot_id_pan/')\n",
        "import load_data\n",
        "from importlib import reload\n",
        "#standard packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#Sk-learn functions\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Utility\n",
        "from collections import Counter\n",
        "#NLTK\n",
        "import nltk\n",
        "#tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#integrated gradients\n",
        "from alibi.explainers import IntegratedGradients\n",
        "\n",
        "np.random.seed(77)\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: alibi in /usr/local/lib/python3.7/dist-packages (0.5.7)\n",
            "Requirement already satisfied: Pillow<9.0,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from alibi) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn<0.25.0,>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from alibi) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.4.1)\n",
            "Requirement already satisfied: tensorflow<2.5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (2.4.1)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (3.2.2)\n",
            "Requirement already satisfied: scikit-image!=0.17.1,<0.19,>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from alibi) (0.16.2)\n",
            "Requirement already satisfied: spacy[lookups]<4.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.19.5)\n",
            "Requirement already satisfied: pandas<2.0.0,>=0.23.3 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.1.5)\n",
            "Requirement already satisfied: attrs<21.0.0,>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (20.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.2; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from alibi) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.20.2->alibi) (1.0.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.12.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.32.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (2.4.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (0.3.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (2.8.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi) (2.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (54.2.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.1.3)\n",
            "Requirement already satisfied: spacy-lookups-data<0.2.0,>=0.0.5; extra == \"lookups\" in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=0.23.3->alibi) (2018.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (0.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (1.28.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (1.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image!=0.17.1,<0.19,>=0.14.2->alibi) (4.4.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.8.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi) (0.4.8)\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import glob\n",
            "import os\n",
            "import xml.etree.ElementTree as ET\n",
            "import xml.etree.ElementTree as ET\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def extract_filename(file):\n",
            "    base = os.path.basename(file)\n",
            "    filename = os.path.splitext(base)[0]\n",
            "    return filename\n",
            "\n",
            "def get_tweets(xml_file):\n",
            "    '''make a list of all tweets in an xml file.'''\n",
            "    mytree = ET.parse(xml_file)\n",
            "    myroot = mytree.getroot()\n",
            "    tweets = []\n",
            "    for x in myroot[0].findall('document'):\n",
            "        tweet = x.text\n",
            "        tweets.append(tweet)\n",
            "    return tweets\n",
            "\n",
            "def read_all_data(path_to_data):\n",
            "    \"\"\"Reads all data from XML and Text into Python\"\"\"\n",
            "    path_to_text = path_to_data + 'truth.txt'\n",
            "    labels = pd.read_csv(path_to_text, delimiter = \"\\:::\",header = None)\n",
            "    labels.rename(columns={0: 'id', 1: 'bot', 2: 'sex'}, inplace=True)\n",
            "    path_to_xml = path_to_data + '/*.xml'\n",
            "    xml = glob.glob(path_to_xml, recursive=False)\n",
            "    ids = []\n",
            "    for i in range(len(xml)):\n",
            "        file = xml[i]\n",
            "        file_id = extract_filename(file)\n",
            "        ids.append(file_id)\n",
            "    mytree = ET.parse(xml[0])\n",
            "    myroot = mytree.getroot()\n",
            "    all_tweets = []\n",
            "    for i in range(len(ids)):\n",
            "        tweets = get_tweets(xml[i])\n",
            "        filename = ids[i]\n",
            "        for i in range(len(tweets)):\n",
            "            pair = [filename, tweets[i]]\n",
            "            all_tweets.append(pair)\n",
            "    tweet_df = pd.DataFrame(all_tweets)\n",
            "    tweet_df = tweet_df.rename(columns = {0:\"tweet_id\", 1:'tweets'})\n",
            "    tweet_df['bot'] = tweet_df['tweet_id'].map(labels.set_index('id')['bot'])\n",
            "    return tweet_df\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvwI4v457SEe"
      },
      "source": [
        "### Load all data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7qQkVV65-OK",
        "outputId": "9ccde2d6-b271-43e1-cbf6-b6f12bcca674"
      },
      "source": [
        "reload(load_data)\n",
        "path_to_train = '/content/gdrive/My Drive/w266_bot_id_final_project/bot_id_pan/pan_bot_id/pan19-author-profiling-training-2019-02-18/en/'\n",
        "train_data = load_data.read_all_data(path_to_train)\n",
        "\n",
        "path_to_early_bird = '/content/gdrive/My Drive/w266_bot_id_final_project/bot_id_pan/pan_bot_id/pan19-author-profiling-earlybirds-20190320/en/'\n",
        "early_bird_data = load_data.read_all_data(path_to_early_bird)\n",
        "\n",
        "path_to_test = '/content/gdrive/My Drive/w266_bot_id_final_project/bot_id_pan/pan_bot_id/pan19-author-profiling-test-2019-04-29/en/'\n",
        "test_data = load_data.read_all_data(path_to_test)\n",
        "\n",
        "# X_train_base = pd.read_csv('/content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/all_data/X_train.csv')\n",
        "# X_dev_base = pd.read_csv('/content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/all_data/X_dev.csv')\n",
        "# X_test_base = pd.read_csv('/content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/all_data/X_test.csv')\n",
        "# X_earlybird_base = pd.read_csv('/content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/all_data/X_earlybird.csv')\n",
        "# y_train_base = pd.read_csv('/content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/all_data/y_train.csv')\n",
        "# y_dev_base = pd.read_csv('/content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/all_data/y_dev.csv')\n",
        "# y_test_base = pd.read_csv('/content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/all_data/y_test.csv')\n",
        "# y_early_bird_base = pd.read_csv('/content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/all_data/y_earlybird.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/w266_bot_id_final_project/bot_id_pan/load_data.py:29: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  labels = pd.read_csv(path_to_text, delimiter = \"\\:::\",header = None)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt32-QmJ7kbn"
      },
      "source": [
        "## Converts Bot Column into a Binary Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urUsn0Pw5-ON"
      },
      "source": [
        "def convert_binary_str_text(column):\n",
        "    if column == \"bot\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "train_data['bot'] = train_data['bot'].apply(convert_binary_str_text)\n",
        "early_bird_data['bot'] = early_bird_data['bot'].apply(convert_binary_str_text)\n",
        "test_data['bot'] = test_data['bot'].apply(convert_binary_str_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1335-ARZ5-ON"
      },
      "source": [
        "# y_train_base = y_train_base['bot']\n",
        "# X_train_base = X_train_base['tweets']\n",
        "# y_early_bird_base = y_dev_base['bot']\n",
        "# X_earlybird_base = X_dev_base['tweets']\n",
        "# y_test_base = y_test_base['bot']\n",
        "# X_test_base = X_test_base['tweets']\n",
        "\n",
        "\n",
        "y_train_base = train_data['bot']\n",
        "X_train_base = train_data['tweets']\n",
        "y_early_bird_base = early_bird_data['bot']\n",
        "X_earlybird_base = early_bird_data['tweets']\n",
        "y_test_base = test_data['bot']\n",
        "X_test_base = test_data['tweets']\n",
        "X_train_base, X_dev_base, y_train_base, y_dev_base = train_test_split(X_train_base, y_train_base, test_size = 0.33, random_state = 37)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVAUsu-WVTHP"
      },
      "source": [
        "#convert data to csv\n",
        "# X_train_base.to_csv('X_train.csv')\n",
        "# X_dev_base.to_csv('X_dev.csv')\n",
        "# y_train_base.to_csv('y_train.csv')\n",
        "# y_dev_base.to_csv('y_dev.csv')\n",
        "# X_earlybird_base.to_csv('X_earlybird.csv')\n",
        "# y_early_bird_base.to_csv('y_earlybird.csv')\n",
        "# y_test_base.to_csv('y_test.csv')\n",
        "# X_test_base.to_csv('X_test.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ9rewA3BXzj"
      },
      "source": [
        "### Checking Max Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ieu0EgYZ5-OO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b146874-23b5-4e31-804a-89a4914d07c6"
      },
      "source": [
        "train_data['Num_words_text'] = train_data['tweets'].apply(lambda x:len(str(x).split())) \n",
        "test_data['Num_words_text'] = test_data['tweets'].apply(lambda x:len(str(x).split())) \n",
        "\n",
        "max_train_sentence_length  = train_data['Num_words_text'].max()\n",
        "max_test_sentence_length  = test_data['Num_words_text'].max()\n",
        "\n",
        "# X_train_base['Num_words_text'] = X_train_base['tweets'].apply(lambda x:len(str(x).split())) \n",
        "# X_test_base['Num_words_text'] = X_test_base['tweets'].apply(lambda x:len(str(x).split())) \n",
        "\n",
        "# max_train_sentence_length  = X_train_base['Num_words_text'].max()\n",
        "# max_test_sentence_length  = X_test_base['Num_words_text'].max()\n",
        "\n",
        "print('Train Max Sentence Length :'+str(max_train_sentence_length))\n",
        "print('Test Max Sentence Length :'+str(max_test_sentence_length))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Max Sentence Length :97\n",
            "Test Max Sentence Length :90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Jqb3tABcQn"
      },
      "source": [
        "### Sets number of words for tokenizer and check if working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yesriPqY5-OO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c860e29d-a851-4e4f-b77c-ef34b69a1d49"
      },
      "source": [
        "NUM_WORDS=100000\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS,lower=True, oov_token=\"unk\")\n",
        "tokenizer.fit_on_texts(X_train_base)\n",
        "\n",
        "print(str(tokenizer.texts_to_sequences(['xuzz how are you'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[35512, 63, 23, 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu2EO8rE5-OP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51d5571-6189-40e2-96ee-a9215aeaf986"
      },
      "source": [
        "sequences_train = tokenizer.texts_to_sequences(X_train_base)\n",
        "sequences_dev=tokenizer.texts_to_sequences(X_dev_base)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test_base)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 373131 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17JOeyLkBjPE"
      },
      "source": [
        "### Pads Sequences and Turns y variable to categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ9VJxUz5-OQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d765c1d-a308-4b75-a726-6b80ebbca67d"
      },
      "source": [
        "X_train = pad_sequences(sequences_train, maxlen = 100)\n",
        "X_dev = pad_sequences(sequences_dev,maxlen=100)\n",
        "X_test = pad_sequences(sequences_test,maxlen = 100)\n",
        "\n",
        "Y_train = to_categorical(np.asarray(y_train_base))\n",
        "Y_dev = to_categorical(np.asarray(y_dev_base))\n",
        "Y_test = to_categorical(np.asarray(y_test_base))\n",
        "\n",
        "print('Shape of X train and X dev tensor and X test tensor:', X_train.shape,X_dev.shape, X_test.shape)\n",
        "print('Shape of label train and dev tensor and test tensor:', Y_train.shape,Y_dev.shape, Y_dev.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X train and X dev tensor and X test tensor: (276040, 100) (135960, 100) (264000, 100)\n",
            "Shape of label train and dev tensor and test tensor: (276040, 2) (135960, 2) (135960, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0RYgIsS7tj-"
      },
      "source": [
        "## Load pre-trained word vectors from google news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsAbQ5kD5-OQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3485103a-a378-4cf2-9134-4c2ac790c405"
      },
      "source": [
        "import gensim.downloader as api\n",
        "path = api.load(\"word2vec-google-news-300\", return_path = True)\n",
        "word_vectors = KeyedVectors.load_word2vec_format(path, binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY6qYNVE5-OR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db329049-9d15-49c6-e14b-c5ca56287bb3"
      },
      "source": [
        "word_vectors['great']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.17773438e-02,  2.08007812e-01, -2.84423828e-02,  1.78710938e-01,\n",
              "        1.32812500e-01, -9.96093750e-02,  9.61914062e-02, -1.16699219e-01,\n",
              "       -8.54492188e-03,  1.48437500e-01, -3.34472656e-02, -1.85546875e-01,\n",
              "        4.10156250e-02, -8.98437500e-02,  2.17285156e-02,  6.93359375e-02,\n",
              "        1.80664062e-01,  2.22656250e-01, -1.00585938e-01, -6.93359375e-02,\n",
              "        1.04427338e-04,  1.60156250e-01,  4.07714844e-02,  7.37304688e-02,\n",
              "        1.53320312e-01,  6.78710938e-02, -1.03027344e-01,  4.17480469e-02,\n",
              "        4.27246094e-02, -1.10351562e-01, -6.68945312e-02,  4.19921875e-02,\n",
              "        2.50000000e-01,  2.12890625e-01,  1.59179688e-01,  1.44653320e-02,\n",
              "       -4.88281250e-02,  1.39770508e-02,  3.55529785e-03,  2.09960938e-01,\n",
              "        1.52343750e-01, -7.32421875e-02,  2.16796875e-01, -5.76171875e-02,\n",
              "       -2.84423828e-02, -3.60107422e-03,  1.52343750e-01, -2.63671875e-02,\n",
              "        2.13623047e-02, -1.51367188e-01,  1.04003906e-01,  3.18359375e-01,\n",
              "       -1.85546875e-01,  3.68652344e-02, -1.10839844e-01, -3.17382812e-02,\n",
              "       -1.01562500e-01, -1.21093750e-01,  3.22265625e-01, -7.32421875e-02,\n",
              "       -1.52343750e-01,  2.67578125e-01, -1.50390625e-01, -1.23046875e-01,\n",
              "        1.07910156e-01,  6.68945312e-02, -2.13623047e-02, -1.00585938e-01,\n",
              "       -2.05078125e-01,  1.17675781e-01,  6.15234375e-02,  6.78710938e-02,\n",
              "        1.06933594e-01, -7.71484375e-02, -1.52343750e-01, -4.24194336e-03,\n",
              "       -1.45507812e-01,  2.53906250e-01,  4.80957031e-02,  9.71679688e-02,\n",
              "       -8.36181641e-03,  1.12792969e-01,  5.34667969e-02,  1.79443359e-02,\n",
              "       -5.63964844e-02, -3.30078125e-01, -9.76562500e-02,  1.42578125e-01,\n",
              "       -1.37695312e-01,  2.20947266e-02,  1.00097656e-01, -5.71289062e-02,\n",
              "       -1.56250000e-01, -6.37817383e-03, -9.37500000e-02, -4.68750000e-02,\n",
              "        8.59375000e-02,  3.06640625e-01, -1.11328125e-01, -1.94335938e-01,\n",
              "       -2.08007812e-01,  8.10546875e-02, -4.19921875e-02, -8.30078125e-02,\n",
              "       -1.04003906e-01,  2.92968750e-01,  2.39257812e-02, -3.85742188e-02,\n",
              "        3.56445312e-02, -1.04980469e-01, -6.54296875e-02,  2.79296875e-01,\n",
              "       -1.16210938e-01, -1.45874023e-02,  3.84765625e-01, -7.81250000e-02,\n",
              "       -2.92968750e-02, -1.35742188e-01, -5.39550781e-02, -5.49316406e-02,\n",
              "       -8.10546875e-02, -2.88085938e-02,  8.34960938e-02,  2.73437500e-01,\n",
              "       -6.20117188e-02, -4.78515625e-02, -1.09252930e-02, -1.13769531e-01,\n",
              "       -1.09863281e-01,  2.02148438e-01, -1.28906250e-01, -6.68945312e-02,\n",
              "       -2.67578125e-01,  9.61914062e-02,  1.04003906e-01, -1.69921875e-01,\n",
              "        5.56640625e-02,  1.54296875e-01,  8.05664062e-02,  2.19726562e-01,\n",
              "       -2.27539062e-01,  1.10351562e-01, -8.11767578e-03, -5.63964844e-02,\n",
              "       -9.03320312e-02, -7.76367188e-02, -3.61328125e-02,  3.61328125e-02,\n",
              "        1.58203125e-01, -1.56250000e-01,  2.26562500e-01,  2.85156250e-01,\n",
              "       -5.51757812e-02,  3.53515625e-01, -1.20605469e-01,  1.05957031e-01,\n",
              "        3.11279297e-02, -1.91406250e-01, -2.31445312e-01, -1.11816406e-01,\n",
              "        2.38037109e-03,  7.51953125e-02, -1.28784180e-02,  1.00585938e-01,\n",
              "        4.45312500e-01, -2.77343750e-01,  6.68945312e-02, -8.10546875e-02,\n",
              "        6.39648438e-02,  1.85546875e-02, -1.11328125e-01,  9.76562500e-02,\n",
              "        2.06054688e-01, -1.30859375e-01,  2.39257812e-02,  1.10839844e-01,\n",
              "        8.05664062e-02, -1.52343750e-01,  4.85229492e-03,  1.84326172e-02,\n",
              "       -9.17968750e-02, -2.41210938e-01,  8.39843750e-02, -1.00585938e-01,\n",
              "       -1.54296875e-01,  2.75878906e-02, -1.64062500e-01, -1.01562500e-01,\n",
              "       -6.07299805e-03,  1.33514404e-03, -2.53906250e-01,  3.14453125e-01,\n",
              "        1.31835938e-01, -1.31835938e-01,  2.17285156e-02, -1.56250000e-01,\n",
              "       -1.46484375e-01, -5.12695312e-02, -1.20605469e-01, -2.15820312e-01,\n",
              "        3.10058594e-02,  1.30859375e-01,  9.71679688e-02,  5.67626953e-03,\n",
              "        2.20947266e-02,  1.26953125e-01, -1.24511719e-02,  6.15234375e-02,\n",
              "       -2.23388672e-02,  2.50000000e-01, -7.17773438e-02,  1.58203125e-01,\n",
              "       -7.27539062e-02,  1.97753906e-02,  8.85009766e-03, -9.08203125e-02,\n",
              "        3.63281250e-01, -9.03320312e-02,  2.41699219e-02, -1.39770508e-02,\n",
              "       -5.10253906e-02,  2.40478516e-02,  5.88989258e-03, -1.02050781e-01,\n",
              "       -8.85009766e-03,  3.05175781e-02, -7.81250000e-02, -1.27929688e-01,\n",
              "        3.85742188e-02,  2.86865234e-02, -2.28515625e-01, -1.25122070e-02,\n",
              "        1.54296875e-01,  9.13085938e-02,  1.05468750e-01, -6.44531250e-02,\n",
              "       -1.28906250e-01, -1.02050781e-01, -2.16064453e-02, -3.29589844e-02,\n",
              "        7.47070312e-02,  3.78417969e-02,  7.42187500e-02, -1.23901367e-02,\n",
              "       -4.68750000e-02,  4.88281250e-03,  1.03515625e-01, -8.69140625e-02,\n",
              "       -2.26562500e-01, -2.53906250e-01,  3.58886719e-02,  4.45312500e-01,\n",
              "        5.56640625e-02,  1.59179688e-01,  2.71484375e-01, -1.08398438e-01,\n",
              "        6.25000000e-02, -5.59082031e-02, -2.50000000e-01, -1.55273438e-01,\n",
              "       -6.83593750e-02, -1.39648438e-01, -1.59179688e-01, -1.79443359e-02,\n",
              "        2.12402344e-02,  7.37304688e-02,  1.30859375e-01, -8.05664062e-02,\n",
              "        2.99072266e-02,  1.55639648e-02, -1.66015625e-01,  1.50390625e-01,\n",
              "       -6.77490234e-03,  1.01318359e-02,  1.14746094e-01, -1.48437500e-01,\n",
              "       -4.58984375e-02, -1.39648438e-01, -1.73828125e-01, -4.27246094e-02,\n",
              "       -5.81054688e-02,  5.22460938e-02, -1.11328125e-01,  8.44726562e-02,\n",
              "       -2.55126953e-02,  1.40625000e-01, -1.81640625e-01,  1.72119141e-02,\n",
              "       -1.37695312e-01, -1.47705078e-02, -1.14746094e-02,  6.44531250e-02,\n",
              "       -2.89062500e-01, -4.80957031e-02, -1.99218750e-01, -7.12890625e-02,\n",
              "        6.44531250e-02, -1.67968750e-01, -2.08740234e-02, -1.42578125e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofJp_8eG5-OR"
      },
      "source": [
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i>=NUM_WORDS:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnHPN2UC5-OS"
      },
      "source": [
        "del(word_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvqpyYq7BqtZ"
      },
      "source": [
        "### Develop Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4QjCyGk5-OS"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TdGypg35-OT"
      },
      "source": [
        "sequence_length = 100\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
        "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
        "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "reshape = Reshape((3*num_filters,))(flatten)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout) #should this be sigmoid?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbBWJy2l5-OT"
      },
      "source": [
        "# this creates a model that includes\n",
        "model = Model(inputs, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXGa63ka5-OU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d57f13d0-f829-44ab-f06f-115eb3eb0eb4"
      },
      "source": [
        "#change batch size back to 16 for actual model, done to speed it up\n",
        "adam = Adam(lr=1e-3)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['acc'])\n",
        "callbacks = [EarlyStopping(monitor='val_loss')]\n",
        "model.fit(X_train, Y_train, batch_size=26, epochs=3, verbose=1, validation_data=(X_dev, Y_dev), \n",
        "          callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "10617/10617 [==============================] - 1737s 162ms/step - loss: 0.4764 - acc: 0.8653 - val_loss: 0.3102 - val_acc: 0.9242\n",
            "Epoch 2/3\n",
            "10617/10617 [==============================] - 1706s 161ms/step - loss: 0.2784 - acc: 0.9337 - val_loss: 0.2688 - val_acc: 0.9319\n",
            "Epoch 3/3\n",
            "10617/10617 [==============================] - 1729s 163ms/step - loss: 0.2338 - acc: 0.9442 - val_loss: 0.2534 - val_acc: 0.9319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f969fe2f650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUE4vAHv5-OU"
      },
      "source": [
        "y_hat = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0EwfyRf5-OV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b39809e-682e-496e-80be-0932b414bc55"
      },
      "source": [
        "# score, acc = model.evaluate(X_test, Y_test,\n",
        "#                             batch_size=100)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 100, 300)     30000000    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 100, 300, 1)  0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 98, 1, 100)   90100       reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 97, 1, 100)   120100      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 96, 1, 100)   150100      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 1, 1, 100)    0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3, 1, 100)    0           max_pooling2d[0][0]              \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 300)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 300)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            602         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 30,360,902\n",
            "Trainable params: 30,360,902\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtmcGtNzYCMx",
        "outputId": "18a148ee-e8d4-471e-e617-3bef8bdcffc2"
      },
      "source": [
        "model_fname = 'CNN'\n",
        "my_wd = '/content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/'\n",
        "\n",
        "model.save(os.path.join(my_wd, model_fname))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/w266_bot_id_final_project/bot_id_pan/CNN/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTld-SPpdq_f"
      },
      "source": [
        "model.save(\"cnn.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "947oDFuJdslg"
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXytyHR4eB6i"
      },
      "source": [
        "model = load_model('cnn.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQbWe8IWBQ0w"
      },
      "source": [
        "### Accuracy for CNN was 78%"
      ]
    }
  ]
}