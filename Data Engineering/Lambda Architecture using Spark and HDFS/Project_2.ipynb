{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 2: Publishing Assessments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Linux Commands and Instructions for Project**\n",
    "\n",
    "#### 1) Command to Bring Up the Cluster \n",
    "    docker-compose up -d\n",
    "\n",
    "#### 2) Command to Create the Kafka Topic Assessments \n",
    "    docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "\n",
    "#### 3) Command to Publish the Assessments Json Data to the Kafka Topic Using Kafkacat\n",
    "    docker-compose exec mids bash -c \"cat /w205/project-2-hfarb/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessmentsâ€\n",
    "\n",
    "#### 4) Create a symbolic link in the Spark container to the /205 mount point\n",
    "     docker-compose exec spark bash\n",
    "     ln -s /w205 w205\n",
    "     exit\n",
    "#### 5) Run an Enhanced Version of the Pyspark Command Line to Target Jupyter Notebook\n",
    "     docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "#### 6)  Change 0.0.0.0 to the External Ip Address for your Google Cloud Virtual Machine and Open Ingonito Browser.\n",
    "     Open a new Google Chrome browser incognito window, and copy and paste the URL with the modified ip address from    and the Jupyter Notebook should come up.\n",
    "#### 7) Command to Shutdown the Cluster\n",
    "     docker-compose down\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Subscribe to the Kafka Topic and Create Data Frames and Temporary Tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a Data Frame by Subscribing to the Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     0|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     1|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     2|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     3|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     4|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     5|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     6|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     7|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     8|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     9|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    10|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    11|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    12|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    13|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    14|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    15|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    16|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    17|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    18|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    19|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert the Json Data to a String "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Convert the String Json Data into a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
      "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
      "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
      "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
      "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
      "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Create Temporary Table 'assessments' from Extracted Assessments Data Frame to Allow In Memory Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Business Questions and Answers Using Spark SQL Against MPP Dataframe in Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 What Are the Top 10 Most Taken Exams and the Top 10 Least Taken?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the temporary table 'assessments' to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Most Taken Exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+-----+\n",
      "|exam_name                                                  |count|\n",
      "+-----------------------------------------------------------+-----+\n",
      "|Learning Git                                               |394  |\n",
      "|Introduction to Python                                     |162  |\n",
      "|Introduction to Java 8                                     |158  |\n",
      "|Intermediate Python Programming                            |158  |\n",
      "|Learning to Program with R                                 |128  |\n",
      "|Introduction to Machine Learning                           |119  |\n",
      "|Software Architecture Fundamentals Understanding the Basics|109  |\n",
      "|Beginning C# Programming                                   |95   |\n",
      "|Learning Eclipse                                           |85   |\n",
      "|Learning Apache Maven                                      |80   |\n",
      "+-----------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(exam_name) as count from assessments group by exam_name order by count desc limit 10\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Least Taken Exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-----+\n",
      "|exam_name                                        |count|\n",
      "+-------------------------------------------------+-----+\n",
      "|Nulls, Three-valued Logic and Missing Information|1    |\n",
      "|Native Web Apps for Android                      |1    |\n",
      "|Learning to Visualize Data with D3.js            |1    |\n",
      "|Operating Red Hat Enterprise Linux Servers       |1    |\n",
      "|The Closed World Assumption                      |2    |\n",
      "|Arduino Prototyping Techniques                   |2    |\n",
      "|What's New in JavaScript                         |2    |\n",
      "|Learning Spring Programming                      |2    |\n",
      "|Hibernate and JPA Fundamentals                   |2    |\n",
      "|Understanding the Grails 3 Domain Model          |2    |\n",
      "+-------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(exam_name) as count from assessments group by exam_name order by count limit 10\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 What Are the Exams with the Highest Average Scores and the Exams with the Lowest Average Scores?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_exam_correct_total(x):\n",
    "    '''Unroll the [sequences][counts] dictionary and check it piece meal to make sure it exists before referencing it.\n",
    "       Add correct and total values to my_list and also add the relevant exam names.\n",
    "       '''\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"],\n",
    "                           \"exam\": raw_dict[\"exam_name\"],}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a data frame from the assessments data frame by applying the my_lambda_exam_correct_total function \n",
    "my_exam_correct_total = assessments.rdd.flatMap(my_lambda_exam_correct_total).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create temporary table 'cet' to allow in memory queries\n",
    "my_exam_correct_total.registerTempTable('cet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------+-----+\n",
      "|correct|exam                                       |total|\n",
      "+-------+-------------------------------------------+-----+\n",
      "|2      |Normal Forms and All That Jazz Master Class|4    |\n",
      "|1      |Normal Forms and All That Jazz Master Class|4    |\n",
      "|3      |The Principles of Microservices            |4    |\n",
      "|2      |The Principles of Microservices            |4    |\n",
      "|3      |Introduction to Big Data                   |4    |\n",
      "|5      |Learning Git                               |5    |\n",
      "|1      |Git Fundamentals for Web Developers        |1    |\n",
      "|5      |Introduction to Python                     |5    |\n",
      "|4      |Intermediate Python Programming            |4    |\n",
      "|0      |Introduction to Python                     |5    |\n",
      "+-------+-------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show what the 'cet' temporary table looks like\n",
    "spark.sql(\"select * from cet limit 10\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Exams with Highest Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+-----------------+\n",
      "|exam                                                                   |avg_score        |\n",
      "+-----------------------------------------------------------------------+-----------------+\n",
      "|Nulls, Three-valued Logic and Missing Information                      |100.0            |\n",
      "|The Closed World Assumption                                            |100.0            |\n",
      "|Learning to Visualize Data with D3.js                                  |100.0            |\n",
      "|Learning SQL for Oracle                                                |97.72727272727273|\n",
      "|Introduction to Java 8                                                 |87.59493670886073|\n",
      "|Introduction to Amazon Web Services (AWS) - EC2 Deployment Fundamentals|83.33333333333334|\n",
      "|Introduction to Apache Spark                                           |83.33333333333334|\n",
      "|Cloud Native Architecture Fundamentals                                 |80.00000000000003|\n",
      "|Getting Ready for Angular 2                                            |80.00000000000001|\n",
      "|Understanding the Grails 3 Domain Model                                |78.57142857142857|\n",
      "+-----------------------------------------------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam, avg(correct / total)*100 as avg_score from cet group by exam order by avg_score desc limit 10\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Exams with Lowest Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------+\n",
      "|exam                                       |avg_score         |\n",
      "+-------------------------------------------+------------------+\n",
      "|Client-Side Data Storage for Web Developers|20.0              |\n",
      "|View Updating                              |25.0              |\n",
      "|Native Web Apps for Android                |25.0              |\n",
      "|Arduino Prototyping Techniques             |33.33333333333333 |\n",
      "|Mastering Advanced Git                     |36.029411764705884|\n",
      "|Nullology                                  |37.5              |\n",
      "|Mastering Web Views                        |41.66666666666667 |\n",
      "|Building Web Services with Java            |41.66666666666667 |\n",
      "|Web & Native Working Together              |41.66666666666667 |\n",
      "|Cloud Computing With AWS                   |42.64705882352941 |\n",
      "+-------------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam, avg(correct / total)*100 as avg_score from cet group by exam order by avg_score limit 10\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** 2.3. What are the Exams with the Most Questions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------------+\n",
      "|exam                                      |number_questions|\n",
      "+------------------------------------------+----------------+\n",
      "|Operating Red Hat Enterprise Linux Servers|20              |\n",
      "|Great Bash                                |10              |\n",
      "|Learning Linux System Administration      |8               |\n",
      "|Learning to Program with R                |7               |\n",
      "|Introduction to Data Science with R       |7               |\n",
      "|Being a Better Introvert                  |7               |\n",
      "|Understanding the Grails 3 Domain Model   |7               |\n",
      "|What's New in JavaScript                  |7               |\n",
      "|Arduino Inputs                            |6               |\n",
      "|Arduino Prototyping Techniques            |6               |\n",
      "+------------------------------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(exam), total as number_questions from cet order by number_questions desc limit 10\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Write to HDFS in Parquet Format for each Data Frame Created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_assessments.write.mode(\"overwrite\").parquet(\"/tmp/raw_assessments\")\n",
    "assessments.write.mode(\"overwrite\").parquet(\"/tmp/assessments\")\n",
    "extracted_assessments.write.mode(\"overwrite\").parquet(\"/tmp/extracted_assessments\")\n",
    "my_exam_correct_total.write.mode(\"overwrite\").parquet(\"/tmp/my_exam_correct_total\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
